# ğŸ­ SER (Speech Emotion Recognition)

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-red)
![License](https://img.shields.io/badge/License-MIT-green)
![Status](https://img.shields.io/badge/Status-Development-yellow)

Algoritmo para clasificar sentimientos a partir de la seÃ±al de voz utilizando tÃ©cnicas de aprendizaje profundo.

## ğŸ‘¨â€ğŸ”¬ Autor
- **Nombre:** Jorge Luis Jorrin Coz
- **InstituciÃ³n:** Instituto PolitÃ©cnico Nacional (IPN)
- **Programa:** BEIFI (Beca de EstÃ­mulo Institucional de FormaciÃ³n de Investigadores)
- **Periodo:** Junio 2025
- **Email:** jjorrinc2100@alumno.ipn.mx, jljorrincoz@gmail.com

## ğŸ“ DescripciÃ³n
Este proyecto implementa un sistema de reconocimiento de emociones en el habla utilizando una arquitectura de aprendizaje profundo que combina:
- BiLSTM (Red neuronal recurrente bidireccional)
- Mecanismos de atenciÃ³n (self-attention, multi-head attention, temporal attention)
- CaracterÃ­sticas espectrales (MFCC)
- CaracterÃ­sticas prosÃ³dicas (RMS, pitch)
- X-vectors (opcional)

## CaracterÃ­sticas
- Procesamiento modular de caracterÃ­sticas acÃºsticas
- AtenciÃ³n configurable (single vs. multi-head, self vs. temporal)
- Balanceo de clases
- ValidaciÃ³n cruzada opcional
- Entrenamiento con early stopping
- VisualizaciÃ³n de resultados (curvas de pÃ©rdida, matriz de confusiÃ³n)

## ğŸ“Š Dataset

El proyecto utiliza el dataset IEMOCAP (Interactive Emotional Dyadic Motion Capture) para el entrenamiento y evaluaciÃ³n. IEMOCAP es uno de los datasets mÃ¡s completos y ampliamente utilizados en el campo del reconocimiento de emociones en el habla.

### CaracterÃ­sticas del Dataset
- **Nombre completo:** Interactive Emotional Dyadic Motion Capture Database
- **TamaÃ±o:** ~12 horas de datos audiovisuales
- **Participantes:** 10 actores en sesiones diÃ¡dicas
- **Idioma:** InglÃ©s
- **Tipos de diÃ¡logo:** 
  - Actuados
  - Improvisados
  - Interacciones naturales

### Emociones clasificadas
| EmociÃ³n | Etiqueta | DescripciÃ³n |
|---------|----------|-------------|
| Neutral | neu | Estado emocional equilibrado |
| FrustraciÃ³n | fru | Sentimiento de impotencia o molestia |
| Sorpresa | sur | Asombro o perplejidad |
| Enojo | ang | Ira o molestia intensa |
| Felicidad | hap | Estado de alegrÃ­a o jÃºbilo |
| Tristeza | sad | MelancolÃ­a o pesar |
| ExcitaciÃ³n | exc | Estado de alta activaciÃ³n emocional |
| Miedo | fea | SensaciÃ³n de amenaza o peligro |
| Otros | xxx | Emociones no categorizadas |

### Referencia
> Busso, C., Bulut, M., Lee, C. C., Kazemzadeh, A., Mower, E., Kim, S., ... & Narayanan, S. S. (2008). IEMOCAP: Interactive emotional dyadic motion capture database. Language resources and evaluation, 42(4), 335-359.

## ğŸ› ï¸ Requisitos

```bash
# InstalaciÃ³n de dependencias
pip install -r requirements.txt
```

Principales dependencias:
- ğŸ”¥ PyTorch >= 1.9.0
- ğŸµ Librosa >= 0.8.1
- ğŸ§® NumPy >= 1.19.5
- ğŸ¼ Pandas >= 1.3.0
- ğŸ§ª Scikit-learn >= 0.24.2
- ğŸ“Š Matplotlib >= 3.4.2
- ğŸ—£ï¸ SpeechBrain >= 0.5.12

## ğŸš€ Uso

### PreparaciÃ³n del entorno
1. Clona el repositorio
```bash
git clone https://github.com/jjorrinc2100/SER.git
cd SER
```

2. Instala las dependencias
```bash
pip install -r requirements.txt
```

### ConfiguraciÃ³n del Dataset
1. Descarga el dataset IEMOCAP (requiere solicitud de acceso)
2. Coloca los archivos de audio en la siguiente estructura:
```
IEMOCAP_full_release/
â”œâ”€â”€ Session1/
â”œâ”€â”€ Session2/
â”œâ”€â”€ Session3/
â”œâ”€â”€ Session4/
â””â”€â”€ Session5/
```

### EjecuciÃ³n
```bash
python main.py
```

## ğŸ“ˆ Resultados

### Archivos generados
- `best_model.pt`: Mejor modelo guardado durante el entrenamiento
- `loss_curve.png`: VisualizaciÃ³n de las curvas de pÃ©rdida

### MÃ©tricas y Visualizaciones
El modelo genera automÃ¡ticamente:
- ğŸ“Š MÃ©tricas de precisiÃ³n por validaciÃ³n cruzada (5-fold)
- ğŸ“‰ Curvas de pÃ©rdida de entrenamiento y validaciÃ³n
- ğŸ¯ Matriz de confusiÃ³n
- ğŸ“‹ Reporte detallado de clasificaciÃ³n con:
  - PrecisiÃ³n por clase
  - Recall por clase
  - F1-score
  - Accuracy global

## ğŸ“š Cita

Si utilizas este trabajo en tu investigaciÃ³n, por favor cÃ­talo como:

```bibtex
@misc{jorrin2025ser,
  author = {Jorrin-Coz, Jorge},
  title = {SER: Speech Emotion Recognition using Deep Learning},
  year = {2025},
  publisher = {GitHub},
  institution = {Instituto PolitÃ©cnico Nacional},
  howpublished = {\url{https://github.com/jorrin2018/SER}}
}
```

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para mÃ¡s detalles.
